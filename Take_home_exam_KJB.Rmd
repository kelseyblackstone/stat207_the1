---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(ggplot2)
library(janitor)
library(useful)
library(magrittr)
library(dygraphs)
library(xgboost)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(inspectdf)
library(caret)
library(ranger)
library(rstanarm)
library(knitr)
library(LearnBayes)
library(kableExtra) 
library(bayesplot)
library(extraDistr)
```


```{r}
# loading in data and understanding data format

setwd("~/Documents/STAT 207")
set.seed(666)
covid_dat <- read.delim("Covid19dat.txt", sep = ",")
tail(covid_dat)
head(covid_dat)
str(covid_dat)
 
# we have 4 columns of data with 58 counties  

County.Covid <- covid_dat$County
Pop.Covid <- covid_dat$Population
Death.Covid <- covid_dat$Deaths
Case.Covid <- covid_dat$Total.cases
propDeath.Covid <- Death.Covid/Pop.Covid

```

```{r}

############################### EDA ################################################

ggplot(data = covid_dat, aes(x=County.Covid, y = Case.Covid)) + geom_point()

ggplot(data = covid_dat, aes(x = Case.Covid, y = Death.Covid, color = County.Covid)) + geom_point() +
  xlab("Cases") + ylab("Deaths") + ggtitle("Deaths ~ Infected")

# looking at death proportion histogram
ggplot(data = covid_dat, aes(x = propDeath.Covid)) + geom_histogram(bins = 50, fill="blue")

## from the scatterplot we see a distinct outlier
## lets identify this outlier in the dataset:
covid_dat[covid_dat$Total.cases > 2500,]
covid_dat2 <- covid_dat[-1,]
# Los Angeles has a large number of infected and deaths 

# Histograms of Deaths and Infections
ggplot(data = covid_dat, aes(x = Case.Covid)) + geom_histogram(bins = 58, fill = "purple") + 
  ggtitle("Total Infected in California") + xlab("Number of Cases") + 
  ylab("Frequency")

ggplot(data = covid_dat, aes(x = Death.Covid)) + geom_histogram(bins = 58, fill = "blue") + 
  ggtitle("Total Deaths in California") + xlab("Number of Deaths") + 
  ylab("Frequency")

```


## Part 2 
Obtain the posterior distribution of θ. Explore the results of fitting the California
COVID 19 data using the samples obtained. Assuming that 20% of the population
will become infected, what are the distributions of the number of deaths for each
county?

```{r}
# poster distribution of theta:
y <- covid_dat$Deaths
n <- covid_dat$Total.cases
ncounty <- nrow(covid_dat)
num.infect <- sum(covid_dat$Total.cases)
alpha_post <- sum(y) + 1/2
beta_post <- sum(n) - sum(y) + 1/2

theta = seq(0,1, length=100)
cart <- seq(0,2,.01)

# generate random data from posterior beta
posterior_beta <- rbeta(n=10000, alpha_post, beta_post)
hist(posterior_beta, main = "Posterior: theta | yi", xlab = "posterior theta")

# overlay of posterior beta against prior beta
for(i in 1:length(alpha)){

    plot(cart, cart, type="n",xlim=c(0,1),ylim=c(-1,10),xlab="",
         ylab="Density",xaxs="i",yaxs="i", main="Prior and Posterior Distribution")
    prior_beta = dbeta(cart, 1/2, 1/2)
    post_beta = dbeta(cart, alpha_post, beta_post)
    lines(cart, post_beta, lwd=1)
    lines(cart, prior_beta, col="purple", lwd=1)
    legend("topright",c("Prior","Posterior"),col=c("purple","black"),lwd=2)

}

# posterior density of theta | yi
post_beta_density <- plot(theta, dbeta(theta, shape1 = alpha_post, shape2 = beta_post), ylab="density", type ="l", col=4, main = "Posterior Distribution of theta | yi")

# given that 20% of population will become infected what are the distributions of the number of deaths for each county?


# Binomial Model mean and variance
Y = sum(y) # total deaths
theta.hat = Y/num.infect # proportion of total deaths by total infected 
theta.hat.county <- y/num.infect
var.data = var(y)
mean.binom <- num.infect * theta.hat
var.binom <- num.infect * theta.hat * (1 - theta.hat)
se.binom <- sqrt(var.binom)
var.binom.county <- num.infect * theta.hat.county * (1 - theta.hat.county)
se.binom.county <- sqrt(var.binom.county)
mean.binom.county <- num.infect * theta.hat.county

# standard dev and mean of each county
se.binom.county
mean.binom.county

# total sd error of population 
se.binom

# Compute posterior mean/var from beta distr 
post.mean.binomP1 <- alpha_post/(alpha_post+beta_post)

#################### Posterior Predictive Distribution Binomial Model #########################

# 20% of each county population becomes infected 
total.infect <- .20*covid_dat$Population

# make empty matrix for total number deaths per county
death.mat <- matrix(NA, nrow = 10000, ncol = ncounty)
for (i in 1:nrow(covid_dat)){
    total.death <- total.infect[i]*rbeta(n=10000, alpha_post, beta_post)
    death.mat[,i] <- total.death[i]
} 
county.death.means <- colMeans(death.mat)
ggplot(data = covid_dat) + aes(x = County.Covid, y=county.death.means, fill ="pink") + geom_point(fill = "pink")

# total dead given 20% of population becomes infected
total.infect.hyp <- round(.20*sum(covid_dat$Population))
prob.death <- post.mean.binomP1
total.dead <- post.mean.binomP1*total.infect.hyp

covid_dat$pred.cases <- round(.20*(covid_dat$Population))

# prediction models for counties 1, 10, 30, and 55
pred.data.count1 <- rbbinom(10000,covid_dat$pred.cases[1],rbeta(n=10000, alpha_post, beta_post))
pred.data.count10 <- rbbinom(10000,covid_dat$pred.cases[10],rbeta(n=10000, alpha_post, beta_post))
pred.data.count30 <- rbbinom(10000,covid_dat$pred.cases[30],rbeta(n=10000, alpha_post, beta_post))
pred.data.count55 <- rbbinom(10000,covid_dat$pred.cases[55],rbeta(n=10000, alpha_post, beta_post))

# histograms for the predictive distributions of counties 1, 10, 30, and 55
ggplot() + aes(x = pred.data.count1) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - County 1") + geom_vline(aes(xintercept=covid_dat$Deaths[1]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 150000)) 
ggsave("preddatacount1binom.pdf")

ggplot() + aes(x = pred.data.count10) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - County 10") + geom_vline(aes(xintercept=covid_dat$Deaths[10]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 10000)) 
ggsave("preddatacount10binom.pdf")

ggplot() + aes(x = pred.data.count30) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - County 30") + geom_vline(aes(xintercept=covid_dat$Deaths[30]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 3000)) 
ggsave("preddatacount30binom.pdf")
  
ggplot() + aes(x = pred.data.count55) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - County 55") + geom_vline(aes(xintercept=covid_dat$Deaths[55]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 1000)) 
ggsave("preddatacount55binom.pdf")
  

############################### Leave One Out ################################################

pred.data <- rbbinom(10000,total.infect.hyp,rbeta(n=10000, alpha_post, beta_post))

ggplot() + aes(x = pred.data) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  coord_cartesian(xlim = c(0, 400000)) + ggtitle("Predicted Number of Deaths - Part 1") 

# probability that deaths will be greater than 200000 given prob infected is .20
mean(rbinom(10000,total.infect.hyp,rbeta(n=10000, alpha_post, beta_post)) > 200000)

# leave one out for county 1 <- los angeles 
covid_dat2 # dataset not including LA
covid_dat2$pred.infect <- round(covid_dat2$Population*.20)
pred.data.LOO <- rbbinom(10000,covid_dat2$pred.infect,rbeta(n=10000, alpha_post, beta_post))

pred.data.count30.LOO <- rbbinom(10000,covid_dat2$pred.infect[30],rbeta(n=10000, alpha_post, beta_post))

ggplot() + aes(x = pred.data.count30.LOO) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - County 30") + geom_vline(aes(xintercept=covid_dat$Deaths[30]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 3000)) 
ggsave("preddeathcounty30LOO.pdf")
```

## Model 2: Beta-Binomial
Consider a second model that assumes the possibility that the data are overdispersed. Use a rejection sampling approach to obtain samples from p(µ, τ |y) and apply your
method to fit the California COVID 19 data. Are there any counties that are particularly influential in the analysis of the posterior for µ? Are there any important
differences between the results for this model and those for model (1)
```{r}
# posterior beta-binomial log likelihood 
logbetabinom <- function(theta, data=covid_dat){
          mu <- theta[1]
          tau <- theta[2]
          y <- Death.Covid
          n <- Case.Covid
          N <- nrow(covid_dat)
          outputfunc <- sum(lchoose(n,y) + lbeta(mu * tau + y, 
                        tau * (1 - mu) + n - y)) - N * lbeta(mu * tau, 
                        tau * (1 - mu)) - log(mu*(1-mu)*(1+tau)^2)
          return(outputfunc)
}

mycontour(logbetabinom, c(0.0025, 0.06, 1, 1700), data = covid_dat,
          xlab = "mu", ylab = "tau")

# skew caused by large values of the precision parameter, tau 
# transform mu, tau to the real line using logit(mu) and log(tau)

# log likelihood function of transformed variables
log_transform_betabinom <- function(theta, data)
  {
    eta <- exp(theta[1])/(1+exp(theta[1]))
    thet <- exp(theta[2])
    y <- Death.Covid
    n <- Case.Covid
    N <- nrow(covid_dat)
    outputfunc <- sum(lbeta(eta * thet + y, thet * (1 - eta) + 
                  n - y)) - N * lbeta(eta * thet, thet * (1 - eta)) - 
                  log(eta*(1 - eta) * (1 + thet)^2) + sum(theta)
    return(outputfunc)
}

mycontour(logf = log_transform_betabinom, limits = c(-4, -2.9, 3.5, 8), data = covid_dat,
          xlab = "logit(mu)", ylab = "log(tau)")

# looks like logit(mu) is around -3.5 and log(mu) is around 5

################################# REJECTION SAMPLING ########################################
# following jimmy albert in bayesian computation  <3 

# find approximation to the posterior mean and covariance using laplace
begin <- c(mu = -4, tau = 3)
fit <- LearnBayes::laplace(log_transform_betabinom, mode = begin, data = covid_dat) 
lap_mode <- fit$mode
lap_var <- fit$var #variance-covar matrix

# function to maximize log g(theta|y) - log p(theta) 
betabinT=function(theta,datapar){
  data=datapar$data
  tpar=datapar$par
  d = log_transform_betabinom(theta,data) - dmt(theta, mean = c(tpar$m),
      S = tpar$var, df = tpar$df, log=TRUE)
  return(d)
}

# maximize density 
tpar <- list(m = lap_mode, var = 2*lap_var, df = 4)
datapar <- list(data=covid_dat, par=tpar)
fit1 <- LearnBayes::laplace(betabinT, begin, datapar)
fit1$mode
dmaxx <- betabinT(fit1$mode, datapar)

# Rejection sampling Step; using a multivariate t-density with location lap_mode, scale matrix
# 2 fit$var, and 4 degrees of freedom - to mimic posterior density and bound ratio from above 

thetaRS <- rejectsampling(log_transform_betabinom, tpar, dmax = dmaxx, 
                           n = 10000, data=covid_dat)
dim(thetaRS) # our acceptance rate is around 50%

# overlay RS samples onto contour plot
mycontour(logf = log_transform_betabinom, limits = c(-4, -2.9, 3.5, 8), 
          data = covid_dat, xlab = "logit(mu)", ylab = "log(tau)")
points(thetaRS[,1], thetaRS[,2])
# such a good sampling method !! we see that most of the points we sampled 
# lie within the contour lines. this good! 

RS_mean_t1 <- mean(thetaRS[,1])
RS_mean_t2 <- mean(thetaRS[,2])
mode <- c(RS_mean_t1, RS_mean_t2)
RS_se_t1 <- sd(thetaRS[,1])
RS_se_t2 <- sd(thetaRS[,2])

#converting mode of logit mu back to mu:
exp(RS_mean_t1)/(exp(RS_mean_t1) +1)
#converting mode of logit mu back to mu:
exp(RS_mean_t2)

mu_samplesRS <- exp(thetaRS[,1])/(exp(thetaRS[,1]) +1)
tau_samplesRS <- exp(thetaRS[,2])

hist(thetaRS[,1])
hist(thetaRS[,2])
hist(mu_samplesRS)
hist(tau_samplesRS)

# compare with true posterior
fit1$mode
npar=list(m=fit$mode,v=fit$var)

# show that betabinomial model fixes overdispersion
# un-transforming variables 
mu.hat <- exp(RS_mean_t1)/(exp(RS_mean_t1) + 1)
tau.hat <- exp(RS_mean_t2)

alpha = mu.hat * tau.hat
beta = tau.hat * (1 - mu.hat)

mean.betabin <- num.infect*mu.hat/58 # should be close to theta 
var.betabino <- num.infect*(mu.hat - mu.hat^2) * (tau.hat + num.infect)/(tau.hat + 1)/58
mean.betabin
se.beta.bin.county <- sqrt(var.betabino)
se.beta.bin <- sqrt(var.betabino*58)
mu.hat
se.beta.bin 
```

```{r}
#apply your method to fit the California COVID 19 data. Are there any counties that are particularly influential in the analysis of the posterior for µ?

### leave one out
# in this case, mu is an approximation for theta. we should be seeing if there are any large values of ni or yi that may skew our estimate for µ. 

covid_dat2 # dataset not including LA
covid_dat2$pred.infect <- round(covid_dat2$Population*.20)
pred.data.LOO.bb <- rbbinom(10000,covid_dat2$pred.infect,mu.hat)
covid_dat$pred.cases

pred.data.count30.LOO.bb <- rbbinom(10000,covid_dat2$pred.infect[30],mu.hat)
pred.data.count30.bb <- rbbinom(10000,covid_dat$pred.cases[30],mu.hat)

ggplot() + aes(x = pred.data.count30.bb) + 
  geom_histogram(bins = 100, fill = "orange") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("County 30 - BetaBinom") + geom_vline(aes(xintercept=covid_dat$Deaths[30]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 3000)) 
ggsave("preddeathcounty30bb.pdf")

ggplot() + aes(x = pred.data.count30.LOO.bb) + 
  geom_histogram(bins = 100, fill = "orange") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("County 30 - BetaBinom - LOO") + geom_vline(aes(xintercept=covid_dat$Deaths[30]),
            color="blue", linetype="dashed", size=0.5) + coord_cartesian(xlim = c(0, 3000)) 
ggsave("preddeathcounty30LOObb.pdf")

```

## Consider the hierarchical model

Write the posterior distribution of all model parameters as p(θ, µ, τ |y) = p(θ|µ, τ, y)p(µ, τ |y).
Use this factorization to obtain samples from the posterior distribution of θ, µ and τ .
Are there large differences between the counties? Compare the results from this model
to those obtained from models (1) and (2).
```{r}
# sample from p(mu, tau | y)
# plug in vector of (mu, tau) into p(theta|mu,tau, y)
# sample from p(theta|mu,tau, y)

############################# STEP 1 ##############################################
################## Sample from mu, tau | y ########################################


# log posterior for logit(mu), log(tau) | y
logpost_mu_tau <- function(theta, data = covid_dat){
    mu = exp(theta[1])/(1 + exp(theta[1]))
    tau = exp(theta[2])
    y = data[, 3]
    n = data[, 2]
    N = length(y)
    logf <- -2*log(mu*(1-mu)*(1+tau)) + sum(lbeta(mu*tau + y, n - y + tau*(1-mu)) - 
              lbeta(mu*tau, tau*(1-mu))) + theta[1] + theta[2] - 2*log(1+exp(theta[2]))
    return(logf)
}

mycontour(logf = logpost_mu_tau, limits = c(-4, -2.9, 3, 7.2), data = covid_dat,
          xlab = "logit(mu)", ylab = "log(tau)")

############################# STEP 2 ##############################################
################## Sample from theta | mu, tau, y #################################


# find mu hat and tau hat that optimizes this function 
begin <- c(mu = 0, tau = 0)
fit.pt4 <- LearnBayes::laplace(logpost = logpost_mu_tau, mode = begin, data = covid_dat) 
lap_mode.pt4 <- fit.pt4$mode
lap_var.pt4 <- fit.pt4$var #variance-covar matrix

# function to maximize log g(theta|y) - log p(theta) 
# found in albert's book <3 
betabinT=function(theta,datapar){
  data=datapar$data
  tpar=datapar$par
  d = logpost_mu_tau(theta,data) - dmt(theta, mean = c(tpar$m),
      S = tpar$var, df = tpar$df, log=TRUE)
  return(d)
}

#
# maximize density 
tpar.pt4 <- list(m = lap_mode.pt4, var = 2*lap_var.pt4, df = 4)
datapar.pt4 <- list(data=covid_dat, par=tpar.pt4)
fit.new <- LearnBayes::laplace(betabinT, begin, datapar.pt4)
fit.new$mode
dmaxx <- betabinT(fit.new$mode, datapar.pt4)

thetaRS.pt4 <- rejectsampling(logpost_mu_tau, tpar, dmax = dmaxx, 
                           n = 10000, data=covid_dat)
hist(thetaRS[,1])


ggplot() + aes(x = thetaRS[,1]) + geom_histogram(bins = 50, fill = "lightskyblue2") + xlab("Rejection Sampling for logit(mu)")
ggsave("Rejection Sampling for logit(mu).pdf")

# samples of mu:
mu.samp.pt4 <- exp(thetaRS.pt4[,1])/(exp(thetaRS.pt4[,1] + 1))
# samples of tau 
tau.samp.pt4 <- exp(thetaRS.pt4[,2])

# now plug those values of tau, mu into theta | mu, tau, y
alpha.postthet <- y + mu.samp.pt4*tau.samp.pt4
beta.postthet <- n-y+tau.samp.pt4*(1-mu.samp.pt4)
post.thet.samp <- rbeta(n = 10000, shape1 = alpha.postthet, shape2 = beta.postthet)

# making 5100 samples for each of the 58 counties 
# one vector of theta samples 
theta.samples <- matrix(NA, nrow = length(mu.samp.pt4), ncol = ncounty)
for (i in 1:ncounty){
  y <- covid_dat$Deaths
  n <- covid_dat$Total.cases
  alpha.postthet <- y[i] + mu.samp.pt4 * tau.samp.pt4
  beta.postthet <- n[i] - y[i] + tau.samp.pt4 * (1 - mu.samp.pt4)
  
  theta.samples[,i] <- rbeta(n = length(mu.samp.pt4), shape1 = alpha.postthet, 
                             shape2 = beta.postthet)
}

# histogram of theta samples for county 1
ggplot() + aes(x = theta.samples[,1]) + geom_histogram(bins = 50, fill = "orange3") + xlab("Beta Sampling for theta - County 1")
ggsave("BetaSampling_theta.pdf")

# plotting theta means for all counties to compare 
as.numeric(theta.samples)
means <- as.matrix(colMeans(theta.samples))
pdf("postmeanscounty.pdf")
plot(means, xlab="county", ylab = "posterior mean of county", col="blue")
dev.off()
```

## Question 5

Assuming that 20% of the population of California becomes infected, what are the probabilities, under the three different models, that more than 20,000 people will die of COVID 19?

```{r}
# initializing values

total.infect.hyp <- round(.20*sum(covid_dat$Population))
hyp.infect <- covid_dat$pred.infect

############################### Model 1 #########################################

y <- covid_dat$Deaths
ncounty <- nrow(covid_dat)
num.infect <- sum(covid_dat$Total.cases)
alpha_post_binom <- 58*mean(covid_dat$Deaths) + 1/2
beta_post_binom <- num.infect - 58*mean(covid_dat$Deaths) + 1/2

# obtain samples from posterior predictive
theta.binom.pt1 <- rbeta(10000, shape1 = alpha_post_binom, shape2 = beta_post_binom)
hist(theta.binom.pt1)

pred.death.beta <- rbbinom(10000,total.infect.hyp,alpha = alpha_post_binom, beta = beta_post_binom)
pdf("pred_dead_binom.pdf")
hist(x = pred.death.beta, col = "orange", xlab = "number of deaths", main = "Posterior Predictive of Binomial Model")
dev.off()
mean(pred.death.beta > 200000)

############################### Model 2 #########################################

# random samples from posterior predictive: beta-binomial
pred.samps.bbinom <- rbbinom(n = 10000, size = total.infect.hyp, alpha = mu_samplesRS*tau_samplesRS, tau_samplesRS*(1-mu_samplesRS))
pdf("pred_dead_betbin.pdf")
hist(x = pred.samps.bbinom, col = "orange", xlab = "number of deaths", main = "Posterior Predictive of Beta-Binomial Model")
dev.off()

# probability, under the beta-binomial model that deaths will be greater than 200000 in CA
mean(pred.samps.bbinom > 200000)

############################### Model 3 #########################################

# average number infected from hierarchical model
pred.samples.hier <- rbinom(n = 10000, size = total.infect.hyp, prob = theta.samples)
pdf("pred_dead_hier.pdf")
hist(x = pred.samples.hier, col = "orange", xlab = "number of deaths", main = "Posterior Predictive of Beta-Binomial Model")
dev.off()


# probability, under the hierarchical model that deaths will be greater than 200,000 in CA
mean(pred.samples.hier > 200000)

```
