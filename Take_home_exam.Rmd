---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---


1 is easy. 2 is conjugate. 3 was in the hw. 4 might require some work, but then 5 is just application and explanation. You got this!

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(ggplot2)
library(janitor)
library(useful)
library(magrittr)
library(dygraphs)
library(xgboost)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(inspectdf)
library(caret)
library(ranger)
library(knitr)
library(LearnBayes)
library(kableExtra) 
library(bayesplot)
library(rstanarm)
library(extraDistr)
```


```{r}
# loading in data and understanding data format

setwd("~/Documents/STAT 207")
set.seed(666)
covid_dat <- read.delim("Covid19dat.txt", sep = ",")
tail(covid_dat)
head(covid_dat)
str(covid_dat)
 

# we have 4 columns of data with 58 counties  

County.Covid <- covid_dat$County
Pop.Covid <- covid_dat$Population
Death.Covid <- covid_dat$Deaths
Case.Covid <- covid_dat$Total.cases
propDeath.Covid <- Death.Covid/Pop.Covid

```

```{r}
ggplot(data = covid_dat, aes(x = County.Covid, y = propDeath.Covid)) +
    xlab("County") + ylab("proportion of deaths by population") + geom_point()

ggplot(data = covid_dat, aes(x=County.Covid, y = Case.Covid)) + geom_point()

plot(Case.Covid ~ County.Covid)
plot(Case.Covid ~ propDeath.Covid)
ggplot(data = covid_dat, aes(x = Case.Covid, y = Death.Covid, color = County.Covid)) + geom_point() +
  xlab("Cases") + ylab("Deaths") + ggtitle("Deaths ~ Infected")

## from the scatterplot we see a distinct outlier
## lets identify this outlier in the dataset:

covid_dat[covid_dat$Total.cases > 2500,]
covid_dat2 <- covid_dat[-1,]
ggplot(data = covid_dat2, aes(x = Total.cases, y = Deaths, color = County)) + geom_point() +
  xlab("Cases") + ylab("Deaths") + ggtitle("Deaths ~ Infected (no Los Angeles)")

# its that dirty bastard Los Angeles
# maybe we should see if the proportion of cases in LA
# differs significantly from that of the other counties

# looking at death proportion histogram
ggplot(data = covid_dat, aes(x = propDeath.Covid)) + geom_histogram(bins = 50)

# def looks like there is proportions are skewed
# lets try a transformation ?!
ggplot(data = covid_dat, aes(x = propDeath.Covid, fill = County.Covid)) + geom_histogram(bins = 58) +
  xlab("Proportion of Deaths") + ylab("Frequency") + ggtitle("Proportion of Deaths")
ggplot(data = covid_dat, aes(x = propDeath.Covid)) + geom_histogram(bins = 58, fill = "pink") +
  xlab("Proportion of Deaths") + ylab("Frequency") + ggtitle("Proportion of Deaths")

ggplot(data = covid_dat, aes(x = log(Death.Covid))) + geom_histogram(bins = 58)
ggplot(data = covid_dat, aes(x = Case.Covid)) + geom_histogram(bins = 58, fill = "purple") + 
  ggtitle("Total Infected in California") + xlab("Number of Cases") + 
  ylab("Frequency")

ggplot(data = covid_dat, aes(x = Death.Covid)) + geom_histogram(bins = 58, fill = "blue") + 
  ggtitle("Total Deaths in California") + xlab("Number of Deaths") + 
  ylab("Frequency")
```

## Part 2 
Obtain the posterior distribution of θ. Explore the results of fitting the California
COVID 19 data using the samples obtained. Assuming that 20% of the population
will become infected, what are the distributions of the number of deaths for each
county?

```{r}
# poster distribution of theta:
y <- covid_dat$Deaths
n <- covid_dat$Total.cases
ncounty <- nrow(covid_dat)
num.infect <- sum(covid_dat$Total.cases)
alpha_post <- sum(y) + 1/2
beta_post <- sum(n) - sum(y) + 1/2

theta = seq(0,1, length=100)
cart <- seq(0,2,.01)

# generate random data from posterior beta
posterior_beta <- rbeta(n=10000, alpha_post, beta_post)
hist(posterior_beta, main = "Posterior: theta | yi", xlab = "posterior theta")

for(i in 1:length(alpha)){
    dev.new()
    plot(cart, cart, type="n",xlim=c(0,1),ylim=c(-1,10),xlab="",
         ylab="Density",xaxs="i",yaxs="i", main="Prior and Posterior Distribution")
    prior_beta = dbeta(cart, 1/2, 1/2)
    post_beta = dbeta(cart, alpha_post, beta_post)
    lines(cart, post_beta, lwd=1)
    lines(cart, prior_beta, col="purple", lwd=1)
    legend("topright",c("Prior","Posterior"),col=c("purple","black"),lwd=2)

}

post_beta_density <- plot(theta, dbeta(theta, shape1 = alpha_post, shape2 = beta_post), ylab="density", type ="l", col=4, main = "Posterior Distribution of theta | yi")

# explore results of fitting covid data using samples obtained 

# given that 20% of population will become infected what are the distributions of the number of deaths for each county?

pred_covid_infect <- Pop.Covid*0.20
cbind(covid_dat, pred_covid_infect)
est_covid_deaths <- matrix(NA, nrow = ncounty, ncol = 10000)
row.names(est_covid_deaths) <- County.Covid


# Binomial Model mean and variance
Y = sum(y) # total deaths
theta.hat = Y/num.infect # proportion of total deaths by total infected 
theta.hat.county <- y/num.infect
var.data = var(y)
mean.binom <- num.infect * theta.hat
var.binom <- num.infect * theta.hat * (1 - theta.hat)
se.binom <- sqrt(var.binom)
var.binom.county <- num.infect * theta.hat.county * (1 - theta.hat.county)
se.binom.county <- sqrt(var.binom.county)
mean.binom.county <- num.infect * theta.hat.county
# standard dev and mean of each county
se.binom.county
mean.binom.county

# total sd error of population 
se.binom

# Compute posterior mean/var from beta distr 
post.mean <- alpha_post/(alpha_post+beta_post)


# 20% of each county population becomes infected 
total.infect <- .20*covid_dat$Population

# make empty matrix for total number deaths per county
death.mat <- matrix(NA, nrow = 10000, ncol = ncounty)
for (i in 1:nrow(covid_dat)){
    total.death <- total.infect[i]*rbeta(n=10000, alpha_post, beta_post)
    death.mat[,i] <- total.death[i]
} 
county.death.means <- colMeans(death.mat)
ggplot(data = covid_dat) + aes(x = County.Covid, y=county.death.means, fill ="pink") + geom_point(fill = "pink")

# total dead given 20% of population becomes infected
total.infect.allcount <- round(.20*sum(covid_dat$Population))
prob.death <- post.mean
total.dead <- post.mean*total.infect.allcount

covid_dat$pred.cases <- round(.20*(covid_dat$Population))

# histogram of total predicted deaths 

pred.data <- rbbinom(10000,total.infect.allcount,rbeta(n=10000, alpha_post, beta_post))

ggplot() + aes(x = pred.data) + 
  geom_histogram(bins = 100, fill = "green") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Predicted Number of Deaths - Part 1") 

# probability that deaths will be greater than 200000 given prob infected is .20
mean(rbinom(10000,total.infect.allcount,rbeta(n=10000, alpha_post, beta_post)) > 200000)

# histogram of actual deaths:
ggplot(data = covid_dat, aes(x = Death.Covid)) + 
  geom_histogram(bins = 58, fill = "pink") + xlab("number of deaths") + ylab("frequency") + 
  ggtitle("Number of Deaths from COVID19 in California")

mean(covid_dat$Deaths)

# bayesian residuals
```

# mu is going to act like the percentage of death 
```{r}
# posterior beta-binomial log likelihood 
logbetabinom <- function(theta, data=covid_dat){
          mu <- theta[1]
          tau <- theta[2]
          y <- Death.Covid
          n <- Case.Covid
          N <- nrow(covid_dat)
          outputfunc <- sum(lchoose(n,y) + lbeta(mu * tau + y, 
                        tau * (1 - mu) + n - y)) - N * lbeta(mu * tau, 
                        tau * (1 - mu)) - log(mu*(1-mu)*(1+tau)^2)
          return(outputfunc)
}

mycontour(logbetabinom, c(0.0025, 0.06, 1, 1700), data = covid_dat,
          xlab = "logit(mu)", ylab = "log(tau)")

# that precision skew is real bad for large values of tau 
# we gonna transform these betches to the real line using logit(mu) and log(tau)

# log likelihood function of transformed variables
# log(tau) and logit(mu)

log_transform_betabinom <- function(theta, data)
  {
    eta <- exp(theta[1])/(1+exp(theta[1]))
    thet <- exp(theta[2])
    y <- Death.Covid
    n <- Case.Covid
    N <- nrow(covid_dat)
    outputfunc <- sum(lbeta(eta * thet + y, thet * (1 - eta) + 
                  n - y)) - N * lbeta(eta * thet, thet * (1 - eta)) - 
                  log(eta*(1 - eta) * (1 + thet)^2) + sum(theta)
    return(outputfunc)
}

mycontour(logf = log_transform_betabinom, limits = c(-4, -2.9, 3.5, 8), data = covid_dat,
          xlab = "logit(mu)", ylab = "log(tau)")

# looks like logit(mu) is around -3.5 and log(mu) is around 5


```


```{r}
# following jimmy albert in learn bayes <3 

# find approximation to the posterior mean and covariance using laplace
begin <- c(mu = -4, tau = 3)
fit <- laplace(logpost = log_transform_betabinom, mode = begin, data = covid_dat) 
lap_mode <- fit$mode
lap_var <- fit$var #variance-covar matrix

# function to maximize log g(theta|y) - log p(theta) 
betabinT=function(theta,datapar){
  data=datapar$data
  tpar=datapar$par
  d = log_transform_betabinom(theta,data) - dmt(theta, mean = c(tpar$m),
      S = tpar$var, df = tpar$df, log=TRUE)
  return(d)
}

# maximize density 
tpar <- list(m = lap_mode, var = 2*lap_var, df = 4)
datapar <- list(data=covid_dat, par=tpar)
fit1 <- laplace(betabinT, begin, datapar)
fit1$mode
dmaxx <- betabinT(fit1$mode, datapar)

# Rejection sampling
# using a multivariate t density with location lap_mode, scale matrix
# 2 fit$var, and 4 degrees of freedom - to mimic posterior density and bound ratio
# from above 

thetaRS <- rejectsampling(log_transform_betabinom, tpar, dmax = dmaxx, 
                           n = 10000, data=covid_dat)
dim(thetaRS)

mycontour(logf = log_transform_betabinom, limits = c(-4, -2.9, 3.5, 8), 
          data = covid_dat, xlab = "logit(mu)", ylab = "log(tau)")
points(thetaRS[,1], thetaRS[,2])

RS_mean_t1 <- mean(thetaRS[,1])
RS_mean_t2 <- mean(thetaRS[,2])
mode <- c(RS_mean_t1, RS_mean_t2)
RS_se_t1 <- sd(thetaRS[,1])
RS_se_t2 <- sd(thetaRS[,2])

#converting mode of logit mu back to mu:
exp(RS_mean_t1)/(exp(RS_mean_t1) +1)
#converting mode of logit mu back to mu:
exp(RS_mean_t2)

# <3 such a good sampling method !! we see that most of the points we sampled 
# lie within the contour lines. this good! 

hist(thetaRS[,1])
hist(thetaRS[,2])

# compare with true posterior
fit1$mode
npar=list(m=fit$mode,v=fit$var)

# show that betabinomial model fixes overdispersion
# un-transforming variables 
mu.hat <- exp(RS_mean_t1)/(exp(RS_mean_t1) + 1)
tau.hat <- exp(RS_mean_t2)

alpha = mu.hat * tau.hat
beta = tau.hat * (1 - mu.hat)

mean.betabin <- num.infect*mu.hat/58 # should be close to theta 
var.betabino <- num.infect*(mu.hat - mu.hat^2) * (tau.hat + num.infect)/(tau.hat + 1)/58
mean.betabin
se.beta.bin.county <- sqrt(var.betabino)
se.beta.bin <- sqrt(var.betabino*58)
mu.hat
```

```{r}
#apply your method to fit the California COVID 19 data. Are there any counties that are particularly influential in the analysis of the posterior for µ?

### leave one out
# in this case, mu is an approximation for theta. we should be seeing if there are any large values of ni or yi that may skew our estimate for µ. 


```

```{r}
# sample from p(mu, tau | y)
# plug in vector of (mu, tau) into p(theta|mu,tau, y)
# sample from p(theta|mu,tau, y)

############################# STEP 1 ##############################################
################## Sample from mu, tau | y ########################################


# log posterior for mu, tau | y
logpost_mu_tau <- function(theta, data = covid_dat){
    mu = exp(theta[1])/(1 + exp(theta[1]))
    tau = exp(theta[2])
    y = data[, 3]
    n = data[, 2]
    N = length(y)
    logf <- -2*log(mu*(1-mu)*(1+tau)) + sum(lbeta(mu*tau + y, n - y + tau*(1-mu)) - 
              lbeta(mu*tau, tau*(1-mu))) + theta[1] + theta[2] - 2*log(1+exp(theta[2]))
    return(logf)
}

mycontour(logf = logpost_mu_tau, limits = c(-4, -2.9, 3.5, 8), data = covid_dat,
          xlab = "logit(mu)", ylab = "log(tau)")

# find mu hat and tau hat that optimizes this function 
begin <- c(mu = 0, tau = 0)
fit.pt4 <- laplace(logpost = logpost_mu_tau, mode = begin, data = covid_dat) 
lap_mode.pt4 <- fit.pt4$mode
lap_var.pt4 <- fit.pt4$var #variance-covar matrix

# function to maximize log g(theta|y) - log p(theta) 
# found in albert's book <3 
betabinT=function(theta,datapar){
  data=datapar$data
  tpar=datapar$par
  d = logpost_mu_tau(theta,data) - dmt(theta, mean = c(tpar$m),
      S = tpar$var, df = tpar$df, log=TRUE)
  return(d)
}

# maximize density 
tpar.pt4 <- list(m = lap_mode.pt4, var = 2*lap_var.pt4, df = 4)
datapar.pt4 <- list(data=covid_dat, par=tpar.pt4)
fit.new <- laplace(betabinT, begin, datapar.pt4)
fit.new$mode
dmaxx <- betabinT(fit.new$mode, datapar.pt4)

thetaRS.pt4 <- rejectsampling(logpost_mu_tau, tpar, dmax = dmaxx, 
                           n = 10000, data=covid_dat)
hist(thetaRS[,1])
# samples of mu:
mu.samp.pt4 <- exp(thetaRS.pt4[,1])/(exp(thetaRS.pt4[,1] + 1))
# samples of tau 
tau.samp.pt4 <- exp(thetaRS.pt4[,2])

# now plug those values of tau, mu into theta | mu, tau, y
alpha.postthet <- y + mu.samp.pt4*tau.samp.pt4
beta.postthet <- n-y+tau.samp.pt4*(1-mu.samp.pt4)
post.thet.samp <- rbeta(n = 10000, shape1 = alpha.postthet, shape2 = beta.postthet)

hist(mu.samp.pt4)

# making 5100 samples for each of the 58 counties 
# one vector of theta samples 
theta.samples <- matrix(NA, nrow = length(mu.samp.pt4), ncol = ncounty)
for (i in 1:ncounty){
  
  y <- covid_dat$Deaths
  n <- covid_dat$Total.cases
  alpha.postthet <- y[i] + mu.samp.pt4 * tau.samp.pt4
  beta.postthet <- n[i] - y[i] + tau.samp.pt4 * (1 - mu.samp.pt4)
  
  theta.samples[,i] <- rbeta(n = length(mu.samp.pt4), shape1 = alpha.postthet, 
                             shape2 = beta.postthet)
}

as.numeric(theta.samples)

mean.theta <- matrix(NA, nrow=1, ncol = ncounty)
for (i in 1:ncounty){
  val <- mean(theta.samples[,i])
  mean.theta[,i] <- val
}

```

## Assuming that 20% of the population of California becomes infected, what are the probabilities, under the three different models, that more than 20,000 people will die of COVID 19?

### 1. under model 1, probability of y|theta = 0.20 = 20,000
```{r}
# total dead given
total.infect.allcount <- round(.20*sum(covid_dat$Population))
prob.death <- post.mean
total.dead <- post.mean*total.infect.allcount


# histogram of total predicted deaths 
hist(rbbinom(10000,total.infect.allcount,rbeta(n=10000, alpha_post, beta_post)))
mean(rbbinom(10000,total.infect.allcount,rbeta(n=10000, alpha_post, beta_post)) > 220000)
pred.death.beta <- rbbinom(10000,total.infect.allcount,rbeta(n=10000, alpha_post, beta_post))
round(mean(pred.death.beta))
```
2. under model 2, probability of y|theta = 0.20 = 20,000
```{r}
# histogram of deaths under beta-binomial
hist(rbbinom(10000,total.infect.allcount,mu.hat))

# probability that the total number of deaths in california will be greater than 200,000
mean(rbbinom(10000,total.infect.allcount,mu.hat) > 200000)

pred.death.bb <- rbbinom(10000,total.infect.allcount, mu.hat)
round(mean(pred.death.bb))
```


3. under model 3, probability of y|theta = 0.20 = 20,000
```{r}
hist(rbinom(10000,total.infect.allcount, as.numeric(theta.samples)))

# average number infected from hierarchical model
pred.death.hier.bayes <- rbinom(10000,total.infect.allcount, as.numeric(theta.samples))
round(mean(pred.death.hier.bayes))

```



```{r}

```

